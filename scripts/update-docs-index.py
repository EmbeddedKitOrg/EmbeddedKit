#!/usr/bin/env python3
"""
æ–‡æ¡£ç´¢å¼•æ›´æ–°è„šæœ¬
è‡ªåŠ¨æ‰«ææ–‡æ¡£ç»“æ„å¹¶ç”Ÿæˆç´¢å¼•æ–‡ä»¶
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
import hashlib

class DocsIndexer:
    def __init__(self, docs_root='docs'):
        self.docs_root = Path(docs_root)
        self.index = {
            'version': '1.0.0',
            'generated': datetime.now().isoformat(),
            'documents': {},
            'modules': {},
            'api': {},
            'examples': {},
            'tags': {},
            'search_index': []
        }
        
    def scan_markdown_files(self):
        """æ‰«ææ‰€æœ‰Markdownæ–‡ä»¶"""
        for md_file in self.docs_root.rglob('*.md'):
            if md_file.name.startswith('_'):
                continue
                
            relative_path = md_file.relative_to(self.docs_root)
            self.process_file(md_file, relative_path)
    
    def process_file(self, file_path, relative_path):
        """å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æå–å…ƒæ•°æ®
            metadata = self.extract_metadata(content)
            
            # æå–æ ‡é¢˜
            title = self.extract_title(content)
            
            # æå–æ ‡ç­¾
            tags = self.extract_tags(content)
            
            # æå–æ‘˜è¦
            summary = self.extract_summary(content)
            
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
            stats = file_path.stat()
            
            # æ„å»ºæ–‡æ¡£ä¿¡æ¯
            doc_info = {
                'path': str(relative_path).replace('\\', '/'),
                'title': title,
                'summary': summary,
                'tags': tags,
                'metadata': metadata,
                'size': stats.st_size,
                'modified': datetime.fromtimestamp(stats.st_mtime).isoformat(),
                'hash': content_hash,
                'word_count': len(content.split()),
                'reading_time': self.calculate_reading_time(content)
            }
            
            # åˆ†ç±»å­˜å‚¨
            self.categorize_document(doc_info, relative_path)
            
            # æ·»åŠ åˆ°æœç´¢ç´¢å¼•
            self.add_to_search_index(doc_info, content)
            
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    def extract_metadata(self, content):
        """æå–YAMLå‰ç½®å…ƒæ•°æ®"""
        metadata = {}
        yaml_match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
        
        if yaml_match:
            yaml_content = yaml_match.group(1)
            for line in yaml_content.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    metadata[key.strip()] = value.strip()
        
        return metadata
    
    def extract_title(self, content):
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        # ä¼˜å…ˆä»å…ƒæ•°æ®ä¸­è·å–
        metadata = self.extract_metadata(content)
        if 'title' in metadata:
            return metadata['title']
        
        # å¦åˆ™ä»ç¬¬ä¸€ä¸ªH1æ ‡é¢˜è·å–
        h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if h1_match:
            return h1_match.group(1).strip()
        
        return "æœªå‘½åæ–‡æ¡£"
    
    def extract_tags(self, content):
        """æå–æ ‡ç­¾"""
        tags = []
        
        # ä»å…ƒæ•°æ®ä¸­æå–
        metadata = self.extract_metadata(content)
        if 'tags' in metadata:
            tag_str = metadata['tags']
            tags.extend([t.strip() for t in tag_str.split(',')])
        
        # ä»å†…å®¹ä¸­æå–æ ‡è®°çš„æ ‡ç­¾
        tag_matches = re.findall(r'\[æ ‡ç­¾:(.+?)\]', content)
        tags.extend(tag_matches)
        
        # è‡ªåŠ¨æ£€æµ‹æ¨¡å—æ ‡ç­¾
        module_matches = re.findall(r'\[æ¨¡å—:(stable|beta|experimental|deprecated)\]', content)
        tags.extend(module_matches)
        
        return list(set(tags))  # å»é‡
    
    def extract_summary(self, content, max_length=200):
        """æå–æ–‡æ¡£æ‘˜è¦"""
        # ç§»é™¤å…ƒæ•°æ®
        content = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)
        
        # ç§»é™¤æ ‡é¢˜
        content = re.sub(r'^#+\s+.+$', '', content, flags=re.MULTILINE)
        
        # ç§»é™¤ä»£ç å—
        content = re.sub(r'```[\s\S]*?```', '', content)
        content = re.sub(r'`[^`]+`', '', content)
        
        # ç§»é™¤é“¾æ¥å’Œå›¾ç‰‡
        content = re.sub(r'!\[.*?\]\(.*?\)', '', content)
        content = re.sub(r'\[.*?\]\(.*?\)', '', content)
        
        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        content = re.sub(r'[*_~>#\-+]', '', content)
        
        # æ¸…ç†ç©ºç™½
        content = ' '.join(content.split())
        
        # æˆªå–æ‘˜è¦
        if len(content) > max_length:
            content = content[:max_length] + '...'
        
        return content
    
    def calculate_reading_time(self, content):
        """è®¡ç®—é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
        # ä¸­æ–‡çº¦300å­—/åˆ†é’Ÿï¼Œè‹±æ–‡çº¦200è¯/åˆ†é’Ÿ
        chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', content))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', content))
        
        reading_time = (chinese_chars / 300) + (english_words / 200)
        return max(1, round(reading_time))
    
    def categorize_document(self, doc_info, relative_path):
        """å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç±»"""
        path_parts = relative_path.parts
        
        # ä¸»ç´¢å¼•
        self.index['documents'][str(relative_path)] = doc_info
        
        # æ¨¡å—æ–‡æ¡£
        if len(path_parts) > 1 and path_parts[0] == 'modules':
            module_name = path_parts[1].replace('.md', '')
            if module_name not in self.index['modules']:
                self.index['modules'][module_name] = []
            self.index['modules'][module_name].append(doc_info)
        
        # APIæ–‡æ¡£
        elif len(path_parts) > 1 and path_parts[0] == 'api':
            api_name = path_parts[1].replace('.md', '')
            self.index['api'][api_name] = doc_info
        
        # ç¤ºä¾‹æ–‡æ¡£
        elif len(path_parts) > 1 and path_parts[0] == 'examples':
            if 'examples' not in self.index:
                self.index['examples'] = []
            self.index['examples'].append(doc_info)
        
        # æ ‡ç­¾ç´¢å¼•
        for tag in doc_info['tags']:
            if tag not in self.index['tags']:
                self.index['tags'][tag] = []
            self.index['tags'][tag].append({
                'path': doc_info['path'],
                'title': doc_info['title']
            })
    
    def add_to_search_index(self, doc_info, content):
        """æ·»åŠ åˆ°æœç´¢ç´¢å¼•"""
        # æå–æ‰€æœ‰æ ‡é¢˜
        headers = re.findall(r'^#+\s+(.+)$', content, re.MULTILINE)
        
        # æ„å»ºæœç´¢æ¡ç›®
        search_entry = {
            'path': doc_info['path'],
            'title': doc_info['title'],
            'headers': headers,
            'tags': doc_info['tags'],
            'summary': doc_info['summary']
        }
        
        self.index['search_index'].append(search_entry)
    
    def generate_statistics(self):
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        self.index['statistics'] = {
            'total_documents': len(self.index['documents']),
            'total_modules': len(self.index['modules']),
            'total_apis': len(self.index['api']),
            'total_examples': len(self.index.get('examples', [])),
            'total_tags': len(self.index['tags']),
            'total_size': sum(doc['size'] for doc in self.index['documents'].values()),
            'average_reading_time': sum(doc['reading_time'] for doc in self.index['documents'].values()) // max(1, len(self.index['documents']))
        }
    
    def save_index(self, output_file='docs/docs-index.json'):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        self.generate_statistics()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.index, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ–‡æ¡£ç´¢å¼•å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in self.index['statistics'].items():
            print(f"  - {key}: {value}")
    
    def generate_sitemap(self, base_url='https://zuoliangyu.github.io/EmbedKit'):
        """ç”Ÿæˆç«™ç‚¹åœ°å›¾"""
        sitemap_content = ['# ç«™ç‚¹åœ°å›¾\n']
        
        # æŒ‰åˆ†ç±»ç»„ç»‡
        sitemap_content.append('\n## æ ¸å¿ƒæ–‡æ¡£\n')
        for path, doc in self.index['documents'].items():
            if not path.startswith(('modules/', 'api/', 'examples/')):
                sitemap_content.append(f"- [{doc['title']}](/{path})")
        
        if self.index['modules']:
            sitemap_content.append('\n## æ¨¡å—æ–‡æ¡£\n')
            for module, docs in self.index['modules'].items():
                sitemap_content.append(f"\n### {module}\n")
                for doc in docs:
                    sitemap_content.append(f"- [{doc['title']}](/{doc['path']})")
        
        if self.index['api']:
            sitemap_content.append('\n## API æ–‡æ¡£\n')
            for api, doc in self.index['api'].items():
                sitemap_content.append(f"- [{doc['title']}](/{doc['path']})")
        
        if self.index.get('examples'):
            sitemap_content.append('\n## ç¤ºä¾‹ä»£ç \n')
            for doc in self.index['examples']:
                sitemap_content.append(f"- [{doc['title']}](/{doc['path']})")
        
        # æ ‡ç­¾äº‘
        if self.index['tags']:
            sitemap_content.append('\n## æ ‡ç­¾ç´¢å¼•\n')
            for tag, docs in sorted(self.index['tags'].items()):
                sitemap_content.append(f"\n### {tag} ({len(docs)})\n")
                for doc in docs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    sitemap_content.append(f"- [{doc['title']}](/{doc['path']})")
                if len(docs) > 5:
                    sitemap_content.append(f"- ...è¿˜æœ‰ {len(docs) - 5} ä¸ªæ–‡æ¡£")
        
        # ä¿å­˜ç«™ç‚¹åœ°å›¾
        with open(self.docs_root / 'sitemap.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(sitemap_content))
        
        print(f"âœ… ç«™ç‚¹åœ°å›¾å·²ç”Ÿæˆ: {self.docs_root / 'sitemap.md'}")
        
        # ç”ŸæˆXML sitemapï¼ˆç”¨äºæœç´¢å¼•æ“ï¼‰
        self.generate_xml_sitemap(base_url)
    
    def generate_xml_sitemap(self, base_url):
        """ç”ŸæˆXMLæ ¼å¼çš„ç«™ç‚¹åœ°å›¾"""
        xml_content = ['<?xml version="1.0" encoding="UTF-8"?>']
        xml_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
        
        for path, doc in self.index['documents'].items():
            url = f"{base_url}/#{path.replace('.md', '')}"
            xml_content.append('  <url>')
            xml_content.append(f'    <loc>{url}</loc>')
            xml_content.append(f'    <lastmod>{doc["modified"][:10]}</lastmod>')
            xml_content.append('    <changefreq>weekly</changefreq>')
            xml_content.append('    <priority>0.8</priority>')
            xml_content.append('  </url>')
        
        xml_content.append('</urlset>')
        
        with open(self.docs_root / 'sitemap.xml', 'w', encoding='utf-8') as f:
            f.write('\n'.join(xml_content))
        
        print(f"âœ… XMLç«™ç‚¹åœ°å›¾å·²ç”Ÿæˆ: {self.docs_root / 'sitemap.xml'}")
    
    def run(self):
        """è¿è¡Œç´¢å¼•å™¨"""
        print("ğŸ”„ å¼€å§‹æ‰«ææ–‡æ¡£...")
        self.scan_markdown_files()
        self.save_index()
        self.generate_sitemap()
        print("âœ¨ æ–‡æ¡£ç´¢å¼•æ›´æ–°å®Œæˆï¼")

if __name__ == '__main__':
    indexer = DocsIndexer()
    indexer.run()